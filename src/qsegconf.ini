[dictionary]
#核心词典后缀
core_dict=core
#子切分数据名称
info_dict=info
#ngram词频数据后缀
ngram_dict=ngram
#alpha平滑数文件后缀
alpha_dict=alpha
#词频数据后缀
prob_dict=prob
#词缀词典数据后缀
affix_dict=affix
#词条组合查询组合id的数据后缀
group_dict=dict
#词缀属性数据后缀
affix_info_dict=info
#词缀对结合度数据后缀
affix_combination_dict=comb
#人名上下文概率词典数据后缀
name_context_dict=name.context
#人名用字概率词典数据后缀
name_char_dict=name.char
#词性转移词典后缀
pos_trans_dict=postrans
#词性混淆数据后缀
pos_confusion_dict=posconf

[segment]
#词典所在路径
dict_path=./qmodule/segment-2.2.1/sws_data
#词典名称(前缀)
dict_name=z20150911

#用户词典名称
#user_dict_name=u20131231
user_dict_name=u20150824
#用户词条权重级别(1-10)
user_dict_weight_level=5

#原子词识别使用的DFA文件
dfa_file=./qmodule/segment-2.2.1/sws_data/dfa_file_qss.map
dfa_file_small=./qmodule/segment-2.2.1/sws_data/dfa_file_qss_small.map

#细粒度原子词识别使用的DFA文件, 整数作为细粒度
fine_grained_dfa_file_integer=./qmodule/segment-2.2.1/sws_data/dfa_file_fine_grained_qss_integer.map

#搜索粒度的日期时间切分使用的DFA文件
search_grained_dfa_file_datetime=./qmodule/segment-2.2.1/sws_data/dfa_file_fine_grained_qss_datetime.map

name_ngram=./qmodule/segment-2.2.1/sws_data/z20130207.name
jp_name_ngram=./qmodule/segment-2.2.1/sws_data/z20130402.jp_name
fn_name_ngram=./qmodule/segment-2.2.1/sws_data/z20130327.fn_name
not_name_ngram=./qmodule/segment-2.2.1/sws_data/z20130402.total
bnd_ngram=./qmodule/segment-2.2.1/sws_data/z20130603.bnd

#语境词典
contextual_dict=z20130225.contextual

#增加的最大句子长度
max_sentence_len = 1024

#增加的最大词长度
max_word_len = 32

#是否识别长词
recognize_long_tail_word = 1

#是否考虑上下文
is_contextual = 0

#是否做词性标注
#tag long word
do_pos_tag = 1
#tag short word
#do_pos_tag = 2
#tag both long and short word
#do_pos_tag = 3

#词的粒度控制
granularity = long
#granularity = short
#granularity = compound

# 消歧算法
#unigram
disambiguity = 1
#bigram
#disambiguity = 2
#trigram
#disambiguity = 3
#forward_maxmatch
#disambiguity = 4
#part bigram
#disambiguity = 5

# 融合ngram模型
# 不融合
mix_model = 0
# 融合
#mix_model = 1

#平滑算法
#简单赋值平滑
#smoothing=1
#katz smoothing
#smoothing=2
#pos smoothing
smoothing=3

# 超过此长度的混合字符串形式原子词，要进行细粒度切分
atom_word_length_threshold = 4

#最大子串长度
sub_word_len = 3

[postag]
#load pos dict
loaddict=1
#unigram
#postagger=1
#bigram
#postagger=2
#trigram
#postagger=3

[output]
#文本输出词与词之间的分隔符
separator=' ' 
#文本输出短词与短词之间的分隔符
sub_separator='0x01'
#文本输出可省略词之间的分隔符
stem_separator='0x02' 
# 文本输出词与词性之间的分隔符
pos_separator = '/'
#文本输出歧义词之间的分隔符
ambi_separator=' ' 
#sub_seg是否输出词性
pos_tag = 0
#机构名输出是否只用长尾词方式输出专名部分
only_output_org_brand=1
#三字中文名输出是否把姓和名分开
split_cn_name=1
#是否将英文数字串切分为一个粗粒度词
link_alphabet_number_str=1
#是否将数字单独成粗粒度词
split_number_as_long=0
#是否转成简体输出
simplified_chinese=1

[tokenizer]
#是否匹配词典
is_dict_match=1
#是否匹配用户词典
is_user_dict_match=1

#是否匹配数字
is_number_match=1
#是否打开人名识别
is_name_tagger=1
#是否打开机构名识别
is_org_tagger=1

#采用的人名识别模型
name_tagger=language_model
#name_tagger=rule
#name_tagger=crf
#name_tagger=maxent

#是否识别非完整机构名
half_org_tagger=0

#是否识别中文人名
tag_cn_name=1
#是否识别日本人名
tag_jp_name=1
#是否识别音译人名
tag_fn_name=0

[alnumpath]
#是否对自动识别的英文串进行清理, 1为清理，默认为0
is_alnum_clear = 0

[phrase]
#是否打开词缀结合
is_affix_combine=1

[normalize]
#繁简转换
#编码映射文件路径
traddat=./qmodule/segment-2.2.1/sws_data/tradmap.dat
convert_tradition=y
tolower=y
banjiao=y

